\chapter{Dataset Creation and Annotation}

As established in the previous chapter, machine learning model performance depends critically on training data quality and representativeness \cite{vapnik1999nature}. This chapter describes the methodology for constructing and annotating a novel dataset for phishing email emotion classification, following established corpus linguistics principles \cite{mcenery2011corpus}.

\section{Dataset Creation Methodology}

No publicly available datasets exist for fine-grained emotional analysis of phishing emails, necessitating the development of a custom corpus. Following Oxford ``Developing Linguistic Corpora'' guidelines \cite{ota2023guidelines} and TEI principles \cite{tei2023guidelines}, we employed a hybrid approach utilizing an established baseline corpus from \cite{Fernandes2024} as input for systematic LLM-based generation \cite{baroni2009web}.

The baseline corpus contained 480 manually curated email samples with fourteen emotional category annotations, meeting established quality standards \cite{hunston2002corpora}. All samples were pre-validated for structural integrity and consistency, requiring no additional preprocessing.

LLM-based generation used systematic prompt engineering with few-shot learning principles \cite{brown2020language,wang2023synthetic}. Exemplars from the baseline corpus provided conditioning context for the Ollama LLM to generate additional phishing emails:

\begin{tcolorbox}[colback=gray!10, colframe=gray!60, title={Standardized Prompt Template for LLM-based Generation}]
\textbf{Prompt Structure:} Based on these examples of emails expressing `\{emotion\}', generate 1 new realistic phishing email that captures the same emotional tone and psychological manipulation strategy. Ensure the generated content maintains professional linguistic coherence while evoking the target emotion of \{emotion\}.

\textbf{Output Specification:} Generate only the email content with no additional explanatory text or metadata.

\textbf{Contextual Examples:} \textit{[Curated exemplars from baseline corpus]}
\end{tcolorbox}

This methodology produced a balanced corpus of 10,000 samples with uniform distribution across fourteen emotional categories ($\sim$
714 per category). The LLM generation process, conditioned on the baseline corpus exemplars, ensured consistency with established phishing patterns while maintaining emotional category fidelity. The stratified sampling ensures statistical robustness and mitigates class imbalance issues \cite{mohammad2018ethics}, providing sufficient statistical power for reliable model evaluation \cite{cohen1988statistical}.

Below is an example of a generated email for the emotion ``Admiration'':

\begin{tcolorbox}[colback=gray!5, colframe=gray!40, title={Example Email — Admiration}]
\obeylines
\textbf{Subject:} A surprise awaits you — Unlock your exclusive benefits today!

Dear [Recipient],

We hope this message finds you well and thriving! We're thrilled to inform you that our records indicate an outstanding opportunity for you — one that we believe will bring joy and appreciation to your day.

Over the years, you've been a valued member of our community. Your dedication, passion, and unwavering commitment have not gone unnoticed. In recognition of your remarkable journey with us, we're delighted to offer you a special surprise — an exclusive upgrade on your current subscription plan with our esteemed organization.

This gesture is but a small token of admiration for your contributions and the positive impact you've made. We trust this unexpected delight will bring a smile to your face and serve as a reminder that your efforts are truly valued.

To unveil the details of your special benefits, please click on the following link: [suspicious link]

Warmest regards,
[Your Name]
[Your Position/Title]
[Organization Name]
\end{tcolorbox}

\section{Annotation Framework and Guidelines}

We developed a comprehensive annotation framework following established best practices \cite{artstein2017annotation,luedeling2009oxford} to optimize inter-annotator agreement, minimize bias, and establish clear decision criteria for emotional category assignment.

\subsection{Annotation Protocol}

The annotation methodology followed four key protocols \cite{fort2011amazon}:

\begin{enumerate}
    \item \textbf{Multi-label Classification:} Annotators could assign multiple emotional labels to single emails, acknowledging that phishing communications often employ mixed emotional appeals \cite{cialdini2021influence}.
    
    \item \textbf{Mandatory Labeling:} Every email required at least one emotional label to ensure complete coverage and prevent unlabeled instances \cite{krippendorff2018content}.
    
    \item \textbf{Uncertainty Handling:} When emotional determination was ambiguous, annotators assigned the ``UNSURE'' label to prevent forced categorization \cite{aroyo2015truth}.
    
    \item \textbf{Explicit Confirmation:} Annotators confirmed each annotation before proceeding to ensure deliberate decision-making.
\end{enumerate}

\subsection{Emotional Taxonomy}

The annotation schema uses a fifteen-category emotional taxonomy based on established psychological frameworks \cite{ekman1992argument, plutchik2001nature}: \textit{Confusion}, \textit{Surprise}, \textit{Joy}, \textit{Admiration}, \textit{Curiosity}, \textit{Desire}, \textit{Anger}, \textit{Excitement}, \textit{Embarrassment}, \textit{Relief}, \textit{Sadness}, \textit{Caring}, \textit{Fear}, \textit{Neutral}, and \textit{UNSURE}.

Each category was operationally defined with precise criteria and exemplars from phishing communications \cite{mohammad2013crowdsourcing}. The framework balances specificity with annotator reliability, optimized through pilot testing and inter-annotator agreement analysis \cite{artstein2008bias}.

\section{Annotation Implementation}

The annotation phase followed established collaborative annotation principles \cite{fort2011amazon}. All generated samples received explicit emotional category labels during the LLM generation process, with each sample labeled based on the emotional category prompt provided to the model. However, comprehensive manual validation was necessary to ensure accuracy and eliminate systematic biases inherent in synthetic generation \cite{wang2023synthetic}.

The corpus scale (10,000 samples) required multi-annotator processing to enhance reliability and minimize individual bias \cite{krippendorff2018content}. We deployed the Doccano collaborative platform \cite{doccano} on DigitalOcean infrastructure \cite{digitalocean}, following best practices for distributed annotation workflows \cite{hovy2006ontonotes}.

\section{Results and Dataset Statistics}
TODO
