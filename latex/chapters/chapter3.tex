\chapter{Dataset Creation and Annotation}

As presented by the previous chapter, the performance of machine learning models is highly dependent on the quality and quantity of the training data. In this chapter, we describe the process of creating and annotating the dataset used for training and evaluating our model.

\section{Dataset Creation}

Since for this study we only required a dataset of the email body content labeled for its emotions, we created our own dataset by taking the existing dataset used in \cite{Fernandes2024} and feeding it into a \ac{LLM} to generate a larger dataset. 

The dataset used required no preprocessing, as it was already in a clean, well-structured format tailored for our needs. The dataset consisted of approximately 480 samples of email body content, each labeled with one of fourteen possible emotions.

To create our dataset, we took all emails related to a certain emotion and prompted the Ollama \ac{LLM} to generate additional samples by providing context and examples, using the following prompt template:

\begin{tcolorbox}[colback=gray!10, colframe=gray!60, title={Prompt Template}]
Based on these examples of emails expressing '{emotion}', generate 1 new realistic phishing email that captures the same emotional tone and style. Make it convincing and professional, evoking the emotion of {emotion}.

Generate only the email content, no additional text or explanations: \textit{Example emails here}
\end{tcolorbox}


Using this method, we were able to create a new dataset with 10,000 samples, equatily distributed across the fourteen emotions. This expanded dataset provided a robust foundation for training our machine learning models to accurately classify the emotional content of phishing emails.

Below is an example of a generated email for the emotion ``Admiration'':

\begin{tcolorbox}[colback=gray!5, colframe=gray!40, title={Example Email — Admiration}]
\obeylines
\textbf{Subject:} A surprise awaits you — Unlock your exclusive benefits today!

Dear [Recipient],

We hope this message finds you well and thriving! We're thrilled to inform you that our records indicate an outstanding opportunity for you — one that we believe will bring joy and appreciation to your day.

Over the years, you've been a valued member of our community. Your dedication, passion, and unwavering commitment have not gone unnoticed. In recognition of your remarkable journey with us, we're delighted to offer you a special surprise — an exclusive upgrade on your current subscription plan with our esteemed organization.

This gesture is but a small token of admiration for your contributions and the positive impact you've made. We trust this unexpected delight will bring a smile to your face and serve as a reminder that your efforts are truly valued.

To unveil the details of your special benefits, please click on the following link: [suspicious link]

Warmest regards,
[Your Name]
[Your Position/Title]
[Organization Name]
\end{tcolorbox}

\section{Annotation Guidelines}

To ensure consistency and accuracy in the annotation process, we established a comprehensive set of guidelines for annotators to follow when labeling phishing emails with emotional dimensions. These guidelines were designed to standardize the annotation procedure, minimize ambiguity, and facilitate reliable inter-annotator agreement.

\subsection{General Annotation Rules}

The annotation process was governed by four fundamental rules:

\begin{enumerate}
    \item Multiple label selection: Annotators were instructed that multiple emotional labels could be assigned to a single email. When an email conveyed more than one emotional tone, all applicable emotions were to be selected (e.g., an email might simultaneously evoke both fear and curiosity).
    
    \item Mandatory labeling: Every email was required to receive at least one emotional label before being marked as annotated. This rule ensured comprehensive coverage of the dataset and prevented unlabeled instances from remaining in the corpus.
    
    \item Uncertainty handling: When annotators could not confidently determine the emotional tone from the email content, they were instructed to select the ``UNSURE'' label. This category served as a safeguard against forced classifications that might compromise data quality.
    
    \item Annotation confirmation: After selecting appropriate labels, annotators were required to explicitly mark each email as annotated by pressing Enter or clicking the designated icon before proceeding to the next instance. This step ensured deliberate, complete annotations.
\end{enumerate}

\subsection{Emotion Category Definitions}

The annotation schema comprised fifteen distinct emotional categories, each with a precise definition and illustrative example drawn from phishing email contexts. The complete taxonomy included: \textit{Confusion}, \textit{Surprise}, \textit{Joy}, \textit{Admiration}, \textit{Curiosity}, \textit{Desire}, \textit{Anger}, \textit{Excitement}, \textit{Embarrassment}, \textit{Relief}, \textit{Sadness}, \textit{Caring}, \textit{Fear}, \textit{Neutral}, and \textit{UNSURE}. Each category was accompanied by a detailed definition specifying the emotional tone or manipulative intent it represented, along with a representative example from phishing email scenarios to guide annotators in recognizing the emotion within realistic contexts.

These guidelines provided annotators with clear decision-making criteria while acknowledging the complex, often multi-dimensional nature of emotional appeals in phishing communications. The framework balanced specificity with flexibility, allowing for nuanced annotation while maintaining consistency across the dataset.

\section{Dataset Annotation}

With the dataset created, and guidelines set, we proceeded to annotate the dataset. Given that the dataset was generated using a \ac{LLM} with specific prompts, each sample was inherently labeled with the intended emotion. However, to ensure the quality and accuracy of these labels, we conducted a manual review process.

Since it's infeasible to annotate all 10,000 samples with just one annotator, and to eliminate potential biases, we decided on using a annotation plataform called Doccano \cite{doccano} hosted on a DigitalOcean \ac{VM} \cite{digitalocean}. This platform allows multiple annotators to work on the same dataset, providing a collaborative environment for annotation.
