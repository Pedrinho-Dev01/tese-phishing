\chapter{Phishing Detection Model}

\section{Model Selection}
The first step in developing a phishing detection model is selecting an appropriate machine learning algorithm. Based on recent advances in NLP, we selected three transformer-based models: \acs{RoBERTa}\cite{liu2019roberta}, \acs{DeBERTa}\cite{he2021debertadecodingenhancedbertdisentangled}, and \acs{ELECTRA}\cite{clark2020electrapretrainingtextencoders}. These models have demonstrated state-of-the-art performance in various NLP tasks, including text classification and semantic similarity \cite{liu2019roberta, schneider2025comparative, sanh2019distilbert}. Their robust architectures and pretraining strategies make them suitable for the challenging task of phishing email detection.

\section{Dataset Preparation}
The dataset used for training and evaluation consists of a collection of emails labeled as either ``phishing'' or ``non-phishing.'' The data was aggregated and preprocessed to ensure quality and consistency. The final dataset, stored in a CSV file, was split into training, validation, and test sets using stratified sampling to maintain class balance. The splits were as follows: 72\% for training, 8\% for validation, and 20\% for testing. Each email was mapped to a binary label (0 for non-phishing, 1 for phishing) to facilitate supervised learning.

\section{Model Implementation}
The implementation leveraged the Hugging Face Transformers library \cite{wolf2020transformers} for model loading, tokenization, and training. For each model (\acs{RoBERTa}, \acs{DeBERTa}, \acs{ELECTRA}), the following steps were performed:
\begin{itemize}
	\item \textbf{Tokenization:} Emails were tokenized using the respective pretrained tokenizer, with padding and truncation to a maximum length of 512 tokens.
	\item \textbf{Model Fine-tuning:} The pretrained model was fine-tuned for binary sequence classification. Training was performed for up to 8 epochs, with early stopping based on validation performance to prevent overfitting.
	\item \textbf{Optimization:} The \ac{AdamW} optimizer was used with a cosine learning rate scheduler, warmup steps, and label smoothing. Mixed precision training (\acs{FP16}) was enabled to accelerate computation on compatible hardware.
	\item \textbf{Evaluation:} Model performance was monitored using accuracy, precision, recall, and F1-score. The best model checkpoint was selected based on the highest F1-score on the validation set.
\end{itemize}

\section{Evaluation and Results}
%TODO
